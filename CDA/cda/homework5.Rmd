---
title: "cda作业5"
author: "林振炜"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(cdabookdb)
library(cdabookfunc)
library(MASS)
library(dplyr)
```
```{r echo=TRUE}
# get the lines of file
lines<-readLines('GDS5037/data_table.csv')
len <- length(lines)

data_COL=read.csv('GDS5037/data_table.csv',encoding='UTF-8',header=TRUE,nrows = 1)
COL <- colnames(data_COL)
description <- read.csv('GDS5037/description.csv')
denote <- data.frame(ID = c('MMA','control','SA'),Y = c(1,0,1))
data <- merge(description,denote,by = 'ID')
des <- data[,-c(1,2)]


f <- function(nrows,line){
  data <- read.csv('GDS5037/data_table.csv',encoding='UTF-8',
                   header=TRUE,
                   nrows = nrows,
                   skip = (line-1)*nrows)
  # bug happened this skip parameter doesn't work
  # data <- read.csv('GDS5037/data_table.csv',encoding='UTF-8',header=TRUE,nrows = nrows,skip = 1*nrows)
  coln <- as.vector(as.matrix((data[,1])))
  colnames(data) <- COL
  data2 = data
  ID_REF <- colnames(data2)
  ID_REF <- ID_REF[-c(1,2)]
  data3 <- data2[,-c(1,2)]
  data3 <- apply(data3,1,as.numeric)
  data4 <- data.frame(ID_REF,data3)
  colnames(data4) <- c('ID_REF',coln)
  data <- merge(des,data4,by = 'ID_REF');
  return(data)
}


ptm <- proc.time()

nrows <- 1000
c2 <- c(rep(10000000,10))
id <- c(rep(0,10))
lines <- len%/%nrows
# so the bug is (len%/%) need a parenthesis

for (line in 1:lines){
  if(line != lines){
    d <- f(nrows,line)
  }else{
    nrows = (len%%nrows)
    d = f(nrows,line)
  }
  
  coln <- colnames(d)[-c(1,2)]
  for(i in 1:nrows){
    m <- glm(Y~d[,i+2],family = binomial(link = 'logit'),data = d);
    if(m$aic < max(c2)){
      id[which.max(c2)] <- coln[i]
      # the sentence must be ahead the next one, or which.max(c2) would change
      c2[which.max(c2)] <- m$aic
    }
  }
  remove(d)
}
proc.time() - ptm

result <- list(c2,id)

library(readr)

ptm <- proc.time() #用于计算读取耗时


filter <- function (x,pos){
  subset(x, ID_REF %in% result[[2]])
}  #定义筛选条件
df <- read_csv_chunked(file = "GDS5037/data_table.csv", DataFrameCallback$new(filter), chunk_size = 1000,col_names = TRUE, progress = FALSE)
t <- proc.time() - ptm


pre <- function(df){
  coln <- as.vector(as.matrix((df[,1])))
  colnames(df) <- COL
  data2 = df
  ID_REF <- colnames(data2)
  ID_REF <- ID_REF[-c(1,2)]
  data3 <- data2[,-c(1,2)]
  data3 <- apply(data3,1,as.numeric)
  data4 <- data.frame(ID_REF,data3)
  colnames(data4) <- c('ID_REF',coln)
  data <- merge(des,data4,by = 'ID_REF');
  return(data)
  
}


data <- pre(df)

library(Hmisc)#加载包
res2 <- rcorr(as.matrix(data[,-c(1,2)]))
res2

## Create a formula for a model with a large number of variables:
xnam <- paste0(colnames(data)[-c(1,2)])
(fmla <- as.formula(paste("Y ~ ", paste(xnam, collapse= "+"))))


m2.full <- glm(fmla,family = binomial(link = 'logit'),data = data)
m2_step <- step(m2.full, trace = T)

m2.step <- glm(Y ~ A_23_P411162 + A_24_P145163 + A_24_P263330 + A_24_P933458 + A_32_P210622,data = data,family = binomial(link = 'logit'));summary(m2.step)
```


- 'aic'
 [1] 77.12579 80.90096 80.80600 73.79519 73.69083 79.28047 80.01105 80.98891
 [9] 79.38464 75.35709
 
 
- 'id_ref'
 [1] "A_32_P128023" "A_23_P50591"  "A_24_P263330" "A_23_P127128" "A_23_P411162"
 [6] "A_32_P210622" "A_24_P933458" "A_24_P353638" "A_24_P145163" "A_32_P171253"

- the correlation matrix is shown above.

- I choose the ten identifier in question 2, because the correlative is weak among them. The result isn't very good, because the coefficient is not significant.
  I can try interaction.


```{r}
library(cdabookdb)
library(dplyr)
data(horseshoecrabs)
horseshoecrabs <- horseshoecrabs %>%
  mutate(
    psat = as.integer(horseshoecrabs$Satellites > 0),
    # psat--whether to have satellites or not
    Spine_factor = factor(Spine, levels = 3:1),
    # grouping of spine, spine type 3 as the benchmark
    Color_factor = factor(Color, levels = 4:1)
    # grouping of color, color type 4 as the benchmark
  )

m1.full <- glm(
  psat ~ Weight + Spine_factor + Color_factor,
  family = binomial(), data = horseshoecrabs
)
summary(m1.full)
m1_backward <- step(m1.full,direction = 'backward',trace = T)
m1_step <- step(m1.full, trace = T)
summary(m1_step)
```



- textbook5.6

```{r}
(1- 0.092)*0.66+0.53*0.092
```

 - 0.092 means the proportion of people who often drink in overall population is 0.092. versus, it means the proportion of people who rarely drink is 0.908. The sensitivity which equals to 0.53 means that 53% would be forecasted successfully in the prediction when he drinks often in fact. And the spectificity which equals to 0.66 means that 66% would be forecasted successfully in the prediction when he drinks rarely in fact.

- $(1- 0.092)\times 0.66+0.53\times 0.092 = 0.64804$

- (i) when maximize sample predictive power is important, I should choose the model with teh four main effects and the six interaction terms.(ii)when model parsimony is important, I should choose the model with only the four main effects.

## textbook 5.10
```{r warning=FALSE}
library(ResourceSelection)
data("horseshoecrabs")
horseshoecrabs$psat <- as.integer(horseshoecrabs$Satellites > 0)
m <- glm(psat~Weight,data = horseshoecrabs,family = binomial(link = 'logit'));summary(m)
pi0 <- 0.642 # cut-off value
pred_prob <- predict(m, type = "response")
pred_type <- cut(
pred_prob, breaks = c(0, pi0, 1), labels = 0:1,
include.lowest = TRUE
)
classification_table <- table(horseshoecrabs$psat, pred_type)
addmargins(classification_table)
sensitivity <- 45/62;sensitivity
specificity <- 85/173;specificity


#Roc curve and AUC area
library(ROCR)
par(pty = "s")
pred <- prediction(fitted(m), horseshoecrabs$psat)
perf <- performance(pred, "tpr", "fpr")
plot(perf, asp =1, xaxs="i", yaxs="i")
performance(pred,"auc")@y.values[[1]]

hoslem.test(m$y, fitted(m))
a <- horseshoecrabs$Weight^2
##d 
m2 <- glm(psat~poly(Weight,2),data = horseshoecrabs,family = binomial(link = 'logit'));summary(m2)
summary(m)

```

- the sensitivity equals to 0.7258; specificity equals to 0.4913.

- the area under it equals to 0.7380. The closer AUC is to 1.0, the higher the authenticity of the detection method.

- the p_value of the Hosmer-Lemeshow statistic equals to 0.4499 showing that the model goodness-of-fit isn't very good.

- when the predictors are x and x2, the coefficient of this model aren't significant than the one before.

- the AIC of model with only one variable is 199.74, and the AIC of model with two variable is 201.46. It shows that the second is more complicated and is not able to add more information.



## textbook 5.18
```{r}
library(ResourceSelection)
library(cdabookfunc)
library(tidyverse)
library(cdabookdb)
data('smoking_lungcancer_cn')
slc <- data.frame(smoking_lungcancer_cn)
slc <- spread(slc,LungCancer,Freq)
m <- glm(cbind(Yes,No)~factor(Smoking)+factor(City),data = slc, family = binomial(link = 'logit'));summary(m)
# X2 test
df <- nrow(slc) - length(coef(m))
X2 <- sum(resid(m, type = "pearson") ^ 2);X2
x2_pvalue <- 1- pchisq(X2, df)
c(X2 = X2, pvalue = x2_pvalue)

# Check residuals to analyze further the quality of fit
res <- resid(m, type = "pearson")
res[order(res)]
```

- when smoking is no, it choose 1. And the coefficient is significant under the level of 0.0001, so obviously smoking is more likely to lead to cancer.

- the result of Pearson test of goodness of fit shows that it's a good fit, the p-value is 0.0016.

- From the reside of the fit, I find that the sample 8,11,7,12 are significantly different from others. We can consider remove these two sample.


## textbook 5.22
```{r}
d522 <- matrix(data=c(rep(0,4),rep(1,4),seq(10,40,10),seq(60,90,10)),nrow = 8)
plot(d522[,2],d522[,1],xlab = 'variable',ylab = 'y',col = 'red',lty = 1, type="o")
m <- glm(d522[,1]~d522[,2],family = binomial(link = 'logit'));summary(m)

d522 <- matrix(data=c(rep(0,5),rep(1,5),seq(10,50,10),seq(50,90,10)),nrow = 10)
plot(d522[,2],d522[,1],xlab = 'variable',ylab = 'y',col = 'red',lty = 1, type="o")
m <- glm(d522[,1]~d522[,2],family = binomial(link = 'logit'));summary(m)

d522 <- matrix(data=c(rep(0,5),rep(1,5),seq(10,40,10),49.9,50.1,seq(60,90,10)),nrow = 10)
plot(d522[,2],d522[,1],xlab = 'variable',ylab = 'y',col = 'red',lty = 1, type="o")
m <- glm(d522[,1]~d522[,2],family = binomial(link = 'logit'));summary(m)

```

- from the plot, we can know about the slope of the line, if we use logistic model to fit it, is infinite.

- the $\beta$ is  2.363, and the se of it is 5805.939 which is far greater than the estimation of $\beta$

- obviously, it isn't correct, too.

- the se is significant greater than estimation. It isn't rational.





















